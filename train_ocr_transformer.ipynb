{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiGkIzX1cwBe",
        "outputId": "2dcd1aa1-37ae-4615-9e77-7f592a6bcd72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training model"
      ],
      "metadata": {
        "id": "UkX0o6Nulupg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "tKyoNOszhUS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8736c8d-8902-4da1-f50b-5dded41ddb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from transformers import LogitsProcessor\n",
        "\n",
        "\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from tqdm import tqdm  # For progress tracking\n",
        "\n",
        "from torchvision import transforms\n",
        "from datasets import Dataset\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers.onnx import export\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
      ],
      "metadata": {
        "id": "7WtqD_UWaj2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set paths"
      ],
      "metadata": {
        "id": "orHrIs-naD-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pth = \"/content/drive/MyDrive/lp_project/saved\"         # init model path\n",
        "train_data_dir = \"/content/drive/MyDrive/lp_project/train_data\"          # Replace with your train dataset folder path   Note: images should be in train_data with labels\n",
        "save_model_pth = \"/content/drive/MyDrive/lp_project/saved\"    # Replace with your save  folder path\n",
        "test_dataset_path = \"/content/drive/MyDrive/lp_project/test_data\"           # Replace with your test dataset folder path\n"
      ],
      "metadata": {
        "id": "5BQPyR4AaBP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "# Load processor and model\n",
        "processor = TrOCRProcessor.from_pretrained(model_pth)\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_pth)\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_pth)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L02_oaJeIWyp",
        "outputId": "dd06a86e-e654-47d8-e8b1-42168b35ecba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config of the encoder: <class 'transformers.models.deit.modeling_deit.DeiTModel'> is overwritten by shared encoder config: DeiTConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 384,\n",
            "  \"image_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1536,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"deit\",\n",
            "  \"num_attention_heads\": 6,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.47.1\"\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"cross_attention_hidden_size\": 384,\n",
            "  \"d_model\": 256,\n",
            "  \"decoder_attention_heads\": 8,\n",
            "  \"decoder_ffn_dim\": 1024,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 6,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"layernorm_embedding\": true,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"trocr\",\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.47.1\",\n",
            "  \"use_cache\": false,\n",
            "  \"use_learned_position_embeddings\": true,\n",
            "  \"vocab_size\": 64044\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Prepare dataset by extracting LP numbers from filenames\n",
        "def preprocess_data_from_folder(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(data_dir):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            # Extract license plate number from filename (before the first underscore)\n",
        "            lp_number = filename.split(\"_\")[0]\n",
        "            images.append(os.path.join(data_dir, filename))\n",
        "            labels.append(lp_number)\n",
        "\n",
        "    return {\"image_path\": images, \"text\": labels}\n",
        "\n",
        "\n",
        "# Load data\n",
        "data = preprocess_data_from_folder(train_data_dir)\n",
        "\n",
        "# Split the data into 80% train and 20% validation\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    data[\"image_path\"], data[\"text\"], test_size=0.01, random_state=42\n",
        ")\n",
        "\n",
        "# Create train and validation datasets\n",
        "train_data = {\"image_path\": train_images, \"text\": train_labels}\n",
        "val_data = {\"image_path\": val_images, \"text\": val_labels}\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "val_dataset = Dataset.from_dict(val_data)\n"
      ],
      "metadata": {
        "id": "dO1p0hafdzP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    # Geometric transforms (keep plates readable)\n",
        "    transforms.RandomRotation(degrees=15),                # Mild rotation\n",
        "    transforms.RandomAffine(\n",
        "        degrees=0,\n",
        "        translate=(0.1, 0.1),  # Minor shifts\n",
        "        shear=5                 # Gentle slant\n",
        "    ),\n",
        "    transforms.RandomPerspective(\n",
        "        distortion_scale=0.4,   # Moderate warping\n",
        "        p=0.5\n",
        "    ),\n",
        "\n",
        "    # Color/lighting adjustments\n",
        "    transforms.ColorJitter(\n",
        "        brightness=0.2,\n",
        "        contrast=0.2,\n",
        "        saturation=0.1,\n",
        "        hue=0.05\n",
        "    ),\n",
        "\n",
        "    # Noise and blur (apply sparingly)\n",
        "    transforms.RandomApply(\n",
        "        [transforms.GaussianBlur(kernel_size=3)],\n",
        "        p=0.3\n",
        "    ),\n",
        "\n",
        "    # Resizing and normalization\n",
        "    transforms.Resize((224, 224)),           # Critical for ViT/CNNs\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Apply transformations and process images\n",
        "    images = [\n",
        "        transform(Image.open(item['image_path']).convert(\"RGB\")) for item in batch\n",
        "    ]\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    # Process images and text into tensors with padding and truncation\n",
        "    # pixel_values = torch.stack(images)  # Stack all transformed tensors into a single batch tensor\n",
        "    pixel_values = processor(images, return_tensors=\"pt\", padding=True, truncation=True).pixel_values\n",
        "    labels = processor.tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    ).input_ids\n",
        "\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, collate_fn=collate_fn)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "hInm2cSJeBKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define the optimizer and learning rate scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.95)"
      ],
      "metadata": {
        "id": "a5RPfudRdzSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7f4798-d8a1-4c58-92c5-8374aeea9661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the decoder start token ID\n",
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "FXM0ahNqjCII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Initialize scaler for mixed precision\n",
        "scaler = GradScaler()\n",
        "# Define number of epochs\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Enable mixed precision training\n",
        "        with autocast():\n",
        "\n",
        "\n",
        "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "\n",
        "\n",
        "        # Scale loss and backpropagate\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Step optimizer and scaler\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Update running loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Optional: Print loss every N batches\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "    # Update learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    avg_loss = running_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} completed. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "agyCF8y8fOuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and processor\n",
        "model.save_pretrained(save_model_pth)\n",
        "processor.save_pretrained(save_model_pth)\n",
        "print(\"Model and processor saved.\")"
      ],
      "metadata": {
        "id": "dtGLuxnqdzd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LicensePlateLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, processor, max_digits=9):\n",
        "        self.processor = processor\n",
        "        self.max_digits = max_digits\n",
        "        self.digit_token_ids = []\n",
        "        self.char_token_ids = []\n",
        "        for token_id in range(processor.tokenizer.vocab_size):\n",
        "            token = processor.decode([token_id]).strip()\n",
        "            if len(token) == 1:\n",
        "                if token.isdigit():\n",
        "                    self.digit_token_ids.append(token_id)\n",
        "                elif token.isalpha():\n",
        "                    self.char_token_ids.append(token_id)\n",
        "\n",
        "    def __call__(self, input_ids, scores):\n",
        "        current_step = input_ids.shape[1]\n",
        "        batch_size = input_ids.shape[0]\n",
        "\n",
        "        for batch_idx in range(batch_size):\n",
        "            generated_tokens = input_ids[batch_idx].tolist()\n",
        "            digit_count = sum(1 for t in generated_tokens if t in self.digit_token_ids)\n",
        "\n",
        "            if current_step < 3:\n",
        "                for token_id in self.char_token_ids:\n",
        "                    scores[batch_idx, token_id] = -float('inf')\n",
        "            elif current_step == 3:\n",
        "                for token_id in self.digit_token_ids:\n",
        "                    scores[batch_idx, token_id] = -float('inf')\n",
        "\n",
        "            else:\n",
        "                if digit_count >= self.max_digits:\n",
        "                    for token_id in self.digit_token_ids:\n",
        "                        scores[batch_idx, token_id] = -float('inf')\n",
        "\n",
        "        return scores\n"
      ],
      "metadata": {
        "id": "daWwSFBGgs6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "save_data = []\n",
        "def pad_to_square_cv2(image):\n",
        "    # Get the current dimensions of the image\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    # Calculate padding to make the image square\n",
        "    if width != height:\n",
        "        # Determine the padding needed for each side\n",
        "        pad_size = abs(width - height) // 2\n",
        "        if width > height:\n",
        "            # Add padding to the top and bottom\n",
        "            padding = ((pad_size, pad_size), (0, 0), (0, 0))  # Padding for (top, bottom, left, right)\n",
        "        else:\n",
        "            # Add padding to the left and right\n",
        "            padding = ((0, 0), (pad_size, pad_size), (0, 0))  # Padding for (top, bottom, left, right)\n",
        "\n",
        "        # Apply padding using numpy\n",
        "        padded_image = np.pad(image, padding, mode='constant', constant_values=(0, 0))  # Black padding\n",
        "\n",
        "    else:\n",
        "        # No padding needed if the image is already square\n",
        "        padded_image = image\n",
        "\n",
        "    return padded_image\n",
        "\n",
        "\n",
        "all_files = [os.path.join(test_dataset_path, f) for f in os.listdir(test_dataset_path) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
        "\n",
        "\n",
        "# Initialize logits processor with format rules\n",
        "logits_processor = LicensePlateLogitsProcessor(processor)\n",
        "\n",
        "def predict_license_plate(image_path):\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "    pil_image = Image.fromarray(image)  # Convert to PIL Image\n",
        "\n",
        "    # Process image and make prediction\n",
        "    inputs = processor(images=pil_image, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs.pixel_values,\n",
        "        logits_processor=[logits_processor],\n",
        "        max_length=9,  # Assuming license plates have 10 characters\n",
        "        num_beams=10,   # Beam search for better accuracy\n",
        "        early_stopping=True\n",
        "    )\n",
        "    prediction = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Evaluation variables\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Read random images and make predictions\n",
        "for filename in tqdm(all_files, desc=\"Processing Images\"):\n",
        "    # Extract ground truth label from filename\n",
        "    filename___ = filename.split(\"/\")[-1]\n",
        "    ground_truth = filename___.split(\"_\")[0]  # The LP number is before the first underscore\n",
        "\n",
        "    true_labels.append(ground_truth)\n",
        "\n",
        "    # Get prediction\n",
        "    # image_path = os.path.join(dataset_path, filename)\n",
        "    predicted_label = predict_license_plate(filename)\n",
        "    predicted_label = predicted_label.replace(\" \", \"\")\n",
        "    predicted_label = predicted_label.lower()\n",
        "    predicted_labels.append(predicted_label)\n",
        "\n",
        "\n",
        "    name_fn = filename___.split('.')[0]\n",
        "    save_data.append((name_fn, predicted_label))\n",
        "\n",
        "\n",
        "\n",
        "# Calculate metrics\n",
        "# LP-Level Accuracy\n",
        "lp_exact_matches = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "lp_accuracy = lp_exact_matches / len(true_labels)\n",
        "\n",
        "# Print results\n",
        "print(f\"Total Samples: {len(true_labels)}\")\n",
        "print(f\"LP-Level Exact Match Accuracy: {lp_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "jrYQQlzQLCIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save results in .txt"
      ],
      "metadata": {
        "id": "9Wxbiyq7d075"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the data by converting the first item to an integer\n",
        "sorted_data = sorted(save_data, key=lambda x: int(x[0]))"
      ],
      "metadata": {
        "id": "RDWIQjjfIPxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"result_all.txt\", \"w\") as file:\n",
        "    # Write each item in the data list to the file\n",
        "    for item in sorted_data:\n",
        "        file.write(f\"{item[0]}  {item[1]}  \"+\"\\n\")  # Format: number followed by string\n"
      ],
      "metadata": {
        "id": "cUXZ52h2FTxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### export to onnx"
      ],
      "metadata": {
        "id": "6usUMWCdBFzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3si6zFEpFM5E",
        "outputId": "eba0292f-6007-47ec-b8a9-8e250e4903c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.5)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/drive/MyDrive/LP_detection/lp_project/test.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Prepare the image for the model (processor will handle necessary transformations)\n",
        "inputs = processor(images=image, return_tensors=\"pt\").to(device)  # Move inputs to the same device as the model\n",
        "\n",
        "# Create dummy decoder input IDs (a tensor of zeros)\n",
        "decoder_input_ids = torch.zeros((inputs[\"pixel_values\"].shape[0], 1), dtype=torch.long).to(device)\n",
        "\n",
        "# Export the model to ONNX format with opset version 14\n",
        "onnx_output_path = \"/content/drive/MyDrive/LP_detection/lp_project/trocr_model.onnx\"\n",
        "\n",
        "# Define the input names (according to the expected inputs of your model)\n",
        "input_names = [\"pixel_values\", \"decoder_input_ids\"]\n",
        "output_names = [\"logits\"]  # This is the output of the model (logits from the decoder)\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(model,\n",
        "                  (inputs[\"pixel_values\"], decoder_input_ids),  # Provide both image tensor and dummy decoder input\n",
        "                  onnx_output_path,\n",
        "                  input_names=input_names,\n",
        "                  output_names=output_names,\n",
        "                  opset_version=14,  # Use opset version 14\n",
        "                  dynamic_axes={\n",
        "                      \"pixel_values\": {0: \"batch_size\", 2: \"height\", 3: \"width\"},  # dynamic input dimensions\n",
        "                      \"decoder_input_ids\": {0: \"batch_size\"},  # dynamic decoder input dimensions\n",
        "                      \"logits\": {0: \"batch_size\"}})  # dynamic output dimensions\n",
        "\n",
        "print(f\"Model exported to {onnx_output_path}\")"
      ],
      "metadata": {
        "id": "GComo4uBBH6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d965f5-e1e8-44a5-b5b4-397ec5111a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to /content/drive/MyDrive/LP_detection/lp_project/trocr_model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xgFTmRk6Csrm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}